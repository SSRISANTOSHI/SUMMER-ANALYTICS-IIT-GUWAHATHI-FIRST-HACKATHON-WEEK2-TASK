{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c60dc38-42ad-4917-b115-26bbf4c79b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded training data from hacktrain.csv. Shape: (8000, 30)\n",
      "Successfully loaded test data from hacktest.csv. Shape: (2845, 29)\n",
      "Dropped 'Unnamed: 0' column from training data.\n",
      "Dropped 'Unnamed: 0' column from test data.\n",
      "\n",
      "First 5 rows of training data:\n",
      "   ID  class  20150720_N  20150602_N  20150517_N  20150501_N  20150415_N  \\\n",
      "0   1  water    637.5950     658.668   -1882.030    -1924.36     997.904   \n",
      "1   2  water    634.2400     593.705   -1625.790    -1672.32     914.198   \n",
      "2   4  water     58.0174   -1599.160         NaN    -1052.63         NaN   \n",
      "3   5  water     72.5180         NaN     380.436    -1256.93     515.805   \n",
      "4   8  water   1136.4400         NaN         NaN     1647.83    1935.800   \n",
      "\n",
      "   20150330_N  20150314_N  20150226_N  ...  20140610_N  20140525_N  \\\n",
      "0   -1739.990     630.087         NaN  ...         NaN   -1043.160   \n",
      "1    -692.386     707.626   -1670.590  ...         NaN    -933.934   \n",
      "2   -1564.630         NaN     729.790  ...    -1025.88     368.622   \n",
      "3   -1413.180    -802.942     683.254  ...    -1813.95     155.624   \n",
      "4         NaN    2158.980         NaN  ...     1535.00    1959.430   \n",
      "\n",
      "   20140509_N  20140423_N  20140407_N  20140322_N  20140218_N  20140202_N  \\\n",
      "0   -1942.490     267.138         NaN         NaN     211.328   -2203.020   \n",
      "1    -625.385     120.059     364.858     476.972     220.878   -2250.000   \n",
      "2         NaN   -1227.800     304.621         NaN     369.214   -2202.120   \n",
      "3         NaN    -924.073     432.150     282.833     298.320   -2197.360   \n",
      "4    -279.317    -384.915    -113.406    1020.720    1660.650    -116.801   \n",
      "\n",
      "   20140117_N  20140101_N  \n",
      "0    -1180.19     433.906  \n",
      "1    -1360.56     524.075  \n",
      "2         NaN   -1343.550  \n",
      "3         NaN    -826.727  \n",
      "4     -568.05   -1357.140  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "Identified 27 NDVI columns.\n",
      "\n",
      "Performing feature engineering on training data...\n",
      "  Adding statistical features to a DataFrame with 8000 rows...\n",
      "  Statistical features added successfully.\n",
      "Performing feature engineering on test data...\n",
      "  Adding statistical features to a DataFrame with 2845 rows...\n",
      "  Statistical features added successfully.\n",
      "\n",
      "Imputing missing values using median strategy...\n",
      "Missing values in NDVI columns imputed.\n",
      "Combined features for training: (8000, 35)\n",
      "Combined features for testing: (2845, 35)\n",
      "\n",
      "Scaling features using StandardScaler...\n",
      "Features scaled successfully.\n",
      "Encoding target variable 'class'...\n",
      "Original classes: ['farm' 'forest' 'grass' 'impervious' 'orchard' 'water']\n",
      "Target variable encoded.\n",
      "\n",
      "Splitting data into training and validation sets (80/20 split)...\n",
      "Training subset shape: (6400, 35), Validation subset shape: (1600, 35)\n",
      "Initializing and training XGBoost model...\n",
      "Model training complete.\n",
      "\n",
      "Validation Accuracy: 0.9437\n",
      "\n",
      "Making predictions on the test dataset...\n",
      "Predictions generated and decoded.\n",
      "Preparing submission file...\n",
      "\n",
      "Submission file 'submission.csv' has been saved and is ready for submission!\n",
      "Script finished successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "train_file_path = \"hacktrain.csv\"\n",
    "test_file_path = \"hacktest.csv\"\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(train_file_path):\n",
    "        raise FileNotFoundError(f\"Training data not found at: {train_file_path}\")\n",
    "    train_df = pd.read_csv(train_file_path)\n",
    "    print(f\"Successfully loaded training data from {train_file_path}. Shape: {train_df.shape}\")\n",
    "\n",
    "    if not os.path.exists(test_file_path):\n",
    "        raise FileNotFoundError(f\"Test data not found at: {test_file_path}\")\n",
    "    test_df = pd.read_csv(test_file_path)\n",
    "    print(f\"Successfully loaded test data from {test_file_path}. Shape: {test_df.shape}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure 'hacktrain.csv' and 'hacktest.csv' are in the same directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during file loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "if \"Unnamed: 0\" in train_df.columns:\n",
    "    train_df = train_df.drop(columns=[\"Unnamed: 0\"])\n",
    "    print(\"Dropped 'Unnamed: 0' column from training data.\")\n",
    "if \"Unnamed: 0\" in test_df.columns:\n",
    "    test_df = test_df.drop(columns=[\"Unnamed: 0\"])\n",
    "    print(\"Dropped 'Unnamed: 0' column from test data.\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of training data:\")\n",
    "print(train_df.head())\n",
    "\n",
    "ndvi_cols = [col for col in train_df.columns if col.endswith('_N')]\n",
    "print(f\"\\nIdentified {len(ndvi_cols)} NDVI columns.\")\n",
    "\n",
    "def add_statistical_features(df):\n",
    "    print(f\"  Adding statistical features to a DataFrame with {df.shape[0]} rows...\")\n",
    "    df['ndvi_mean'] = df[ndvi_cols].mean(axis=1)\n",
    "    df['ndvi_std'] = df[ndvi_cols].std(axis=1)\n",
    "    df['ndvi_min'] = df[ndvi_cols].min(axis=1)\n",
    "    df['ndvi_max'] = df[ndvi_cols].max(axis=1)\n",
    "    df['ndvi_range'] = df['ndvi_max'] - df['ndvi_min']\n",
    "    df['ndvi_spring_mean'] = df[[col for col in ndvi_cols if col[4:6] in ['03', '04', '05']]].mean(axis=1)\n",
    "    df['ndvi_summer_mean'] = df[[col for col in ndvi_cols if col[4:6] in ['06', '07', '08']]].mean(axis=1)\n",
    "    df['ndvi_autumn_mean'] = df[[col for col in ndvi_cols if col[4:6] in ['09', '10', '11']]].mean(axis=1)\n",
    "    print(\"  Statistical features added successfully.\")\n",
    "    return df\n",
    "\n",
    "print(\"\\nPerforming feature engineering on training data...\")\n",
    "train_df = add_statistical_features(train_df)\n",
    "print(\"Performing feature engineering on test data...\")\n",
    "test_df = add_statistical_features(test_df)\n",
    "\n",
    "print(\"\\nImputing missing values using median strategy...\")\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_ndvi_imputed = imputer.fit_transform(train_df[ndvi_cols])\n",
    "X_test_ndvi_imputed = imputer.transform(test_df[ndvi_cols])\n",
    "print(\"Missing values in NDVI columns imputed.\")\n",
    "\n",
    "additional_features = ['ndvi_mean','ndvi_std','ndvi_min','ndvi_max','ndvi_range','ndvi_spring_mean','ndvi_summer_mean','ndvi_autumn_mean']\n",
    "X_train_combined_features = np.hstack([X_train_ndvi_imputed, train_df[additional_features].values])\n",
    "X_test_combined_features = np.hstack([X_test_ndvi_imputed, test_df[additional_features].values])\n",
    "print(f\"Combined features for training: {X_train_combined_features.shape}\")\n",
    "print(f\"Combined features for testing: {X_test_combined_features.shape}\")\n",
    "\n",
    "print(\"\\nScaling features using StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_combined_features)\n",
    "X_test_scaled = scaler.transform(X_test_combined_features)\n",
    "print(\"Features scaled successfully.\")\n",
    "\n",
    "print(\"Encoding target variable 'class'...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(train_df[\"class\"])\n",
    "print(f\"Original classes: {label_encoder.classes_}\")\n",
    "print(\"Target variable encoded.\")\n",
    "\n",
    "print(\"\\nSplitting data into training and validation sets (80/20 split)...\")\n",
    "X_train_subset, X_val_subset, y_train_subset, y_val_subset = train_test_split(\n",
    "    X_train_scaled, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "print(f\"Training subset shape: {X_train_subset.shape}, Validation subset shape: {X_val_subset.shape}\")\n",
    "\n",
    "print(\"Initializing and training XGBoost model...\")\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train_subset, y_train_subset)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "val_preds_encoded = xgb_model.predict(X_val_subset)\n",
    "val_accuracy = accuracy_score(y_val_subset, val_preds_encoded)\n",
    "print(f\"\\nValidation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nMaking predictions on the test dataset...\")\n",
    "test_preds_encoded = xgb_model.predict(X_test_scaled)\n",
    "test_labels_decoded = label_encoder.inverse_transform(test_preds_encoded)\n",
    "print(\"Predictions generated and decoded.\")\n",
    "\n",
    "print(\"Preparing submission file...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"class\": test_labels_decoded\n",
    "})\n",
    "submission_output_path = \"submission.csv\"\n",
    "submission_df.to_csv(submission_output_path, index=False)\n",
    "print(f\"\\nSubmission file '{submission_output_path}' has been saved and is ready for submission!\")\n",
    "print(\"Script finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846f3ea-ca8f-4035-9c04-dea984c37f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
